package org.apache.spark.util

import java.util

import org.apache.spark.rdd.RDD

import scala.reflect.ClassTag

/**
 * This class allows execution of a function on an RDD and all of its dependencies. This is accomplished by
 * walking the object graph linking these RDDs. This is useful for debugging internal RDD references.
 */
object RDDWalker {
  
  //Keep track of both the RDD and its depth in the traversal graph
  val walkQueue = new util.ArrayDeque[(RDD[_], Int)]
  var visited = new util.HashSet[RDD[_]]
  

  /**
   *
   * Execute the passed function on the underlying RDD
   * * @param rddToWalk - The RDD to traverse along with its dependencies
   * @param func - The function to execute on each node. Returns a string 
   * @return Array[String] - An array of results generated by the traversal function
   * TODO Can there be cycles in RDD dependencies?
   */
  def walk(rddToWalk : RDD[_], func : (RDD[_])=>String): util.ArrayList[String] ={
    
    val results = new util.ArrayList[String]
    //Implement as a queue to perform a BFS
    walkQueue.addFirst(rddToWalk,0)

    while(!walkQueue.isEmpty){
      //Pop from the queue
      val (rddToProcess : RDD[_], depth:Int) = walkQueue.pollFirst()
      if(!visited.contains(rddToProcess)){
        rddToProcess.equals()
        rddToProcess.dependencies.foreach(s => walkQueue.addFirst(s.rdd, depth + 1))
        results.add("Depth: " + depth + " , " + func(rddToProcess))
        visited.add(rddToProcess)
      }
    }
    
    results
  }
  
  
}
